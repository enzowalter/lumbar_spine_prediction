[[842.   1.   2.]
 [ 45.   0.   9.]
 [ 16.   0.  34.]]
Precision: 0.564952802826557 Recall: 0.5309213863060017 F1: nan
Epoch 0 train_loss= 0.5540886063611562 metrics= {'loss': 0.21019162486704288, 'logloss': 0.4814849507586944}
New best model ! classification_spinal_canal_stenosis.pth
--------------------------------------------------
[[838.   4.   3.]
 [ 31.  12.  11.]
 [  8.   3.  39.]]
Precision: 0.7374406196856569 Recall: 0.6508800601108293 F1: 0.6657067622704965
Epoch 1 train_loss= 0.4220056559092457 metrics= {'loss': 0.18648150857983037, 'logloss': 0.40108421799517996}
New best model ! classification_spinal_canal_stenosis.pth
--------------------------------------------------
[[836.   8.   1.]
 [ 28.  12.  14.]
 [  6.   7.  37.]]
Precision: 0.6708517536103743 Recall: 0.62768479383864 F1: 0.6384972129441345
Epoch 2 train_loss= 0.36943145937427024 metrics= {'loss': 0.16931507685574107, 'logloss': 0.38080494092212375}
New best model ! classification_spinal_canal_stenosis.pth
--------------------------------------------------
[[829.  12.   4.]
 [ 21.  17.  16.]
 [  4.   6.  40.]]
Precision: 0.6584030333444854 Recall: 0.687242102626718 F1: 0.6641434300929971
Epoch 3 train_loss= 0.34248770416954794 metrics= {'loss': 0.17392225198906372, 'logloss': 0.34181872846556727}
--------------------------------------------------
[[832.  13.   0.]
 [ 26.  20.   8.]
 [  4.  10.  36.]]
Precision: 0.7383081495205814 Recall: 0.6579080179080179 F1: 0.6947688089676879
Epoch 4 train_loss= 0.3243540534763669 metrics= {'loss': 0.15640704831574617, 'logloss': 0.3369847928173655}
New best model ! classification_spinal_canal_stenosis.pth
--------------------------------------------------
[[811.  30.   4.]
 [ 14.  26.  14.]
 [  2.  10.  38.]]
Precision: 0.6404024949542311 Recall: 0.7089608966532044 F1: 0.6720981269640653
Epoch 5 train_loss= 0.3119044454762588 metrics= {'loss': 0.17999066222543633, 'logloss': 0.3301834810345758}
--------------------------------------------------
[[823.  20.   2.]
 [ 17.  30.   7.]
 [  2.  12.  36.]]
Precision: 0.7350252306883983 Recall: 0.7092965154503617 F1: 0.7202511726652239
Epoch 6 train_loss= 0.3071384552961155 metrics= {'loss': 0.1700251274682307, 'logloss': 0.3262535595049993}
--------------------------------------------------
[[833.   9.   3.]
 [ 30.  16.   8.]
 [  5.   6.  39.]]
Precision: 0.7302764976958526 Recall: 0.6711987727372344 F1: 0.6922148474100673
Epoch 7 train_loss= 0.2854513118736865 metrics= {'loss': 0.1636401874444155, 'logloss': 0.36023806429373506}
--------------------------------------------------
[[826.  17.   2.]
 [ 20.  25.   9.]
 [  2.  12.  36.]]
Precision: 0.709116045276222 Recall: 0.6833486741179049 F1: 0.6958258178930322
Epoch 8 train_loss= 0.27792941928589865 metrics= {'loss': 0.15890800356626117, 'logloss': 0.328838492131749}
--------------------------------------------------
[[823.  19.   3.]
 [ 19.  23.  12.]
 [  2.  14.  34.]]
Precision: 0.6531510369889323 Recall: 0.6494023355561817 F1: 0.6511970760638611
Epoch 9 train_loss= 0.2712201339834537 metrics= {'loss': 0.16972679800278048, 'logloss': 0.34676874326631685}
--------------------------------------------------
[[829.  13.   3.]
 [ 20.  27.   7.]
 [  2.  12.  36.]]
Precision: 0.734720626024974 Recall: 0.6944378698224851 F1: 0.7137803234501348
Epoch 10 train_loss= 0.2801216533732783 metrics= {'loss': 0.15860781624474096, 'logloss': 0.3381791346870129}
--------------------------------------------------
[[823.  21.   1.]
 [ 20.  29.   5.]
 [  3.  12.  35.]]
Precision: 0.7604187508685557 Recall: 0.6925769387307849 F1: 0.7214730863459425
Epoch 11 train_loss= 0.27755861399520987 metrics= {'loss': 0.16324306366192076, 'logloss': 0.35235790707613124}
--------------------------------------------------
[[814.  30.   1.]
 [ 11.  36.   7.]
 [  2.  13.  35.]]
Precision: 0.7359266986564563 Recall: 0.7280924204001126 F1: 0.7238776665165217
Epoch 12 train_loss= 0.2718134583706773 metrics= {'loss': 0.1676743392166093, 'logloss': 0.33975272412887414}
--------------------------------------------------
[[826.  18.   1.]
 [ 17.  33.   4.]
 [  4.  14.  32.]]
Precision: 0.7785786694877604 Recall: 0.6799624307316615 F1: 0.7183045387389486
Epoch 13 train_loss= 0.26111557817384806 metrics= {'loss': 0.1670044489950055, 'logloss': 0.3972186606364191}
--------------------------------------------------
[[814.  28.   3.]
 [ 12.  30.  12.]
 [  1.  13.  36.]]
Precision: 0.664697195191921 Recall: 0.7077749600826524 F1: 0.6835956227201667
Epoch 14 train_loss= 0.24870894639162103 metrics= {'loss': 0.16737303057131134, 'logloss': 0.32507141570383785}
--------------------------------------------------
[[837.   7.   1.]
 [ 24.  28.   2.]
 [  5.  17.  28.]]
Precision: 0.8080484292582929 Recall: 0.609652797345105 F1: 0.6857730130743447
Epoch 15 train_loss= 0.23294541389779203 metrics= {'loss': 0.17530358338235766, 'logloss': 0.4566527758840941}
--------------------------------------------------
[[825.  20.   0.]
 [ 16.  35.   3.]
 [  3.  18.  29.]]
Precision: 0.794484608749687 Recall: 0.6560896653204346 F1: 0.7012199873230982
Epoch 16 train_loss= 0.24187319658849868 metrics= {'loss': 0.16186752332950274, 'logloss': 0.3964554363363421}
--------------------------------------------------
[[830.  14.   1.]
 [ 20.  30.   4.]
 [  2.  16.  32.]]
Precision: 0.7762339804593326 Recall: 0.6647656616887386 F1: 0.7104795903562168
Epoch 17 train_loss= 0.22475874001932714 metrics= {'loss': 0.16120544343511234, 'logloss': 0.3746453904903251}
--------------------------------------------------
[[818.  24.   3.]
 [ 14.  32.   8.]
 [  2.  15.  33.]]
Precision: 0.6974605426322925 Recall: 0.6847475032090417 F1: 0.6867000222668364
Epoch 18 train_loss= 0.22068582467047362 metrics= {'loss': 0.16375450019008542, 'logloss': 0.34056957412104366}
--------------------------------------------------
[[813.  30.   2.]
 [ 13.  33.   8.]
 [  2.  16.  32.]]
Precision: 0.6949923062235351 Recall: 0.6777646285338593 F1: 0.6781423789990322
Epoch 19 train_loss= 0.22675784175690444 metrics= {'loss': 0.16465887641469537, 'logloss': 0.33410912129312076}
--------------------------------------------------
[[733.  12.   0.]
 [171.  34.   0.]
 [ 13.  23.   0.]]
Precision: nan Recall: 0.18794284778897644 F1: nan
Epoch 0 train_loss= 0.7955514581018196 metrics= {'loss': 0.47966589666733, 'logloss': 0.8937368680615333}
New best model ! classification_left_neural_foraminal_narrowing.pth
--------------------------------------------------
[[700.  45.   0.]
 [111.  94.   0.]
 [  6.  24.   6.]]
Precision: 0.8585953398262385 Recall: 0.36047673611923076 F1: 0.43726901215309927
Epoch 1 train_loss= 0.6523888445752961 metrics= {'loss': 0.44152398815928273, 'logloss': 0.8271687036827887}
New best model ! classification_left_neural_foraminal_narrowing.pth
--------------------------------------------------
[[668.  76.   1.]
 [ 77. 115.  13.]
 [  4.  16.  16.]]
Precision: 0.5909000360268718 Recall: 0.5423390417988552 F1: 0.5643060621420662
Epoch 2 train_loss= 0.6052646696047328 metrics= {'loss': 0.44388517560380647, 'logloss': 0.7639679005799312}
--------------------------------------------------
[[700.  45.   0.]
 [ 92.  96.  17.]
 [  4.  11.  21.]]
Precision: 0.621868742207277 Recall: 0.6013594306604618 F1: 0.6077716422521444
Epoch 3 train_loss= 0.5514329740240625 metrics= {'loss': 0.396670510863064, 'logloss': 0.7340558473180834}
New best model ! classification_left_neural_foraminal_narrowing.pth
--------------------------------------------------
[[718.  27.   0.]
 [117.  84.   4.]
 [  6.  16.  14.]]
Precision: 0.7553844711147136 Recall: 0.4769751628484645 F1: 0.5702206733216016
Epoch 4 train_loss= 0.5350555034228476 metrics= {'loss': 0.3919879807866888, 'logloss': 0.8147623751802153}
New best model ! classification_left_neural_foraminal_narrowing.pth
--------------------------------------------------
[[693.  52.   0.]
 [ 83. 110.  12.]
 [  3.  10.  23.]]
Precision: 0.6853204642755093 Recall: 0.6512753756492502 F1: 0.6668724097125622
Epoch 5 train_loss= 0.5080531901822573 metrics= {'loss': 0.3803944469102917, 'logloss': 0.706101100744007}
New best model ! classification_left_neural_foraminal_narrowing.pth
--------------------------------------------------
[[684.  59.   2.]
 [ 81.  96.  28.]
 [  3.   7.  26.]]
Precision: 0.5618504346182918 Recall: 0.6776564371588106 F1: 0.6016221434084377
Epoch 6 train_loss= 0.4844573337023636 metrics= {'loss': 0.3978010229633985, 'logloss': 0.7272429976565663}
--------------------------------------------------
[[679.  62.   4.]
 [ 73. 111.  21.]
 [  3.   9.  24.]]
Precision: 0.5826145202277112 Recall: 0.6658575559868734 F1: 0.6159205263500749
Epoch 7 train_loss= 0.4839569309779851 metrics= {'loss': 0.3874095071446123, 'logloss': 0.6659729509513106}
--------------------------------------------------
[[679.  66.   0.]
 [ 68. 122.  15.]
 [  3.  10.  23.]]
Precision: 0.6512441710336448 Recall: 0.6653155505668192 F1: 0.6579665435205406
Epoch 8 train_loss= 0.45692510333851866 metrics= {'loss': 0.37385811172158323, 'logloss': 0.6773709781239393}
New best model ! classification_left_neural_foraminal_narrowing.pth
--------------------------------------------------
[[699.  45.   1.]
 [ 87.  93.  25.]
 [  4.   6.  26.]]
Precision: 0.5966395418927064 Recall: 0.6763515708023811 F1: 0.6200411611572397
Epoch 9 train_loss= 0.44288278206016624 metrics= {'loss': 0.37637040631951385, 'logloss': 0.692361236863887}
--------------------------------------------------
[[698.  47.   0.]
 [ 83. 105.  17.]
 [  5.   9.  22.]]
Precision: 0.6355426849950581 Recall: 0.6293924914321052 F1: 0.629432855516311
Epoch 10 train_loss= 0.4224291673819489 metrics= {'loss': 0.3837591948603022, 'logloss': 0.7599832207233242}
--------------------------------------------------
[[701.  44.   0.]
 [ 83. 115.   7.]
 [  4.  12.  20.]]
Precision: 0.7425123016226409 Recall: 0.6121590055785505 F1: 0.6682333479393895
Epoch 11 train_loss= 0.4256386889727907 metrics= {'loss': 0.37749427416786974, 'logloss': 0.7335463978116639}
--------------------------------------------------
[[695.  50.   0.]
 [ 80. 109.  16.]
 [  5.   9.  22.]]
Precision: 0.6434905946184142 Recall: 0.6343921406615792 F1: 0.636964941786284
Epoch 12 train_loss= 0.4035022120584047 metrics= {'loss': 0.38035924624260037, 'logloss': 0.7388535363587785}
--------------------------------------------------
[[663.  82.   0.]
 [ 64. 113.  28.]
 [  4.   8.  24.]]
Precision: 0.55234729756439 Recall: 0.6655769395661358 F1: 0.5982907575447031
Epoch 13 train_loss= 0.3955601006077161 metrics= {'loss': 0.38775322367408627, 'logloss': 0.6751127518789164}
--------------------------------------------------
[[669.  76.   0.]
 [ 71. 124.  10.]
 [  4.  12.  20.]]
Precision: 0.6765245051154972 Recall: 0.6185664138520579 F1: 0.6446115032825247
Epoch 14 train_loss= 0.37866250171015714 metrics= {'loss': 0.37558577549591277, 'logloss': 0.7024260320794924}
--------------------------------------------------
[[673.  72.   0.]
 [ 69. 114.  22.]
 [  4.   8.  24.]]
Precision: 0.5949084322594079 Recall: 0.668888213330839 F1: 0.6267243414026551
Epoch 15 train_loss= 0.3787358691009718 metrics= {'loss': 0.3852856836015945, 'logloss': 0.6967121421874375}
--------------------------------------------------
[[692.  52.   1.]
 [ 75. 106.  24.]
 [  4.   7.  25.]]
Precision: 0.5974834504017384 Recall: 0.6772547399491253 F1: 0.6263508715984631
Epoch 16 train_loss= 0.35490112794397527 metrics= {'loss': 0.3857860108099997, 'logloss': 0.7270938041747256}
--------------------------------------------------
[[675.  70.   0.]
 [ 64. 129.  12.]
 [  3.  12.  21.]]
Precision: 0.6682724092376499 Recall: 0.6425585981650804 F1: 0.6547193450098335
Epoch 17 train_loss= 0.3572615533963322 metrics= {'loss': 0.37432376918423743, 'logloss': 0.6700747203277322}
--------------------------------------------------
[[659.  86.   0.]
 [ 56. 137.  12.]
 [  2.  14.  20.]]
Precision: 0.6536036479100936 Recall: 0.6347673352093061 F1: 0.6420378527081674
Epoch 18 train_loss= 0.3397127288545422 metrics= {'loss': 0.3985522465672227, 'logloss': 0.6860837434198833}
--------------------------------------------------
[[692.  53.   0.]
 [ 73. 123.   9.]
 [  5.  10.  21.]]
Precision: 0.7173259919803697 Recall: 0.6374560562480026 F1: 0.6738997027214596
Epoch 19 train_loss= 0.33151283407025844 metrics= {'loss': 0.3749336708837559, 'logloss': 0.7485859975128357}
--------------------------------------------------
Error label nan
Error label nan
Error label nan
Error label nan
Error label nan
Error label nan
[[719.  45.   0.]
 [ 87.  86.   0.]
 [  4.  41.   0.]]
Precision: nan Recall: 0.27647416137414016 F1: nan
Epoch 0 train_loss= 0.7652207868641914 metrics= {'loss': 0.46295191615520237, 'logloss': 0.8323343922905699}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[716.  48.   0.]
 [ 75.  98.   0.]
 [  1.  43.   1.]]
Precision: 0.8487253487253488 Recall: 0.3084299486623731 F1: 0.3110134935636493
Epoch 1 train_loss= 0.6201181755322053 metrics= {'loss': 0.38402628493300867, 'logloss': 0.7213566207510462}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[695.  69.   0.]
 [ 63.  97.  13.]
 [  1.  12.  32.]]
Precision: 0.6928586584668656 Recall: 0.6965025130793977 F1: 0.6946471295781868
Epoch 2 train_loss= 0.5646520208757476 metrics= {'loss': 0.3838039040695302, 'logloss': 0.6604033677528499}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[708.  54.   2.]
 [ 74.  84.  15.]
 [  1.  10.  34.]]
Precision: 0.6722880515983964 Recall: 0.7028602941141149 F1: 0.6850546113484934
Epoch 3 train_loss= 0.5265768882052244 metrics= {'loss': 0.36196287851604014, 'logloss': 0.6365329121655177}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[710.  52.   2.]
 [ 64. 101.   8.]
 [  1.  11.  33.]]
Precision: 0.745371970170243 Recall: 0.7186118232663643 F1: 0.7316414456296803
Epoch 4 train_loss= 0.5081515415071867 metrics= {'loss': 0.35208802643786147, 'logloss': 0.6162322056837178}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[722.  41.   1.]
 [ 86.  74.  13.]
 [  1.  12.  32.]]
Precision: 0.6914889790439548 Recall: 0.6635659931335264 F1: 0.6739777970946802
Epoch 5 train_loss= 0.49199110512902966 metrics= {'loss': 0.34114542920291785, 'logloss': 0.6370819275097422}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[713.  50.   1.]
 [ 65.  96.  12.]
 [  1.  11.  33.]]
Precision: 0.7153958783660356 Recall: 0.7109151423181626 F1: 0.7127012789081115
Epoch 6 train_loss= 0.45459547167147935 metrics= {'loss': 0.33299403145431317, 'logloss': 0.5817094997423969}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[720.  44.   0.]
 [ 71.  95.   7.]
 [  1.  12.  32.]]
Precision: 0.7784886195482221 Recall: 0.6978741024802219 F1: 0.7351297750367404
Epoch 7 train_loss= 0.45864514129027506 metrics= {'loss': 0.3303823219891424, 'logloss': 0.5878857366956523}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[694.  70.   0.]
 [ 57. 103.  13.]
 [  1.  10.  34.]]
Precision: 0.7060246316873453 Recall: 0.7316215186656733 F1: 0.7184846274626931
Epoch 8 train_loss= 0.4365593016797688 metrics= {'loss': 0.3546746958657917, 'logloss': 0.5742232369092083}
--------------------------------------------------
[[710.  53.   1.]
 [ 64. 100.   9.]
 [  1.  13.  31.]]
Precision: 0.7350483647527053 Recall: 0.6915634702064507 F1: 0.7123341423157644
Epoch 9 train_loss= 0.42987450907953306 metrics= {'loss': 0.33031557686223845, 'logloss': 0.584630933855665}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[704.  59.   1.]
 [ 67.  96.  10.]
 [  1.  12.  32.]]
Precision: 0.7197659833277861 Recall: 0.6965338575180297 F1: 0.7078813343519226
Epoch 10 train_loss= 0.4066727380525109 metrics= {'loss': 0.34343232013584024, 'logloss': 0.6214543600178762}
--------------------------------------------------
[[682.  82.   0.]
 [ 48. 109.  16.]
 [  0.  12.  33.]]
Precision: 0.6717165359969481 Recall: 0.7265884424768304 F1: 0.6972957686877194
Epoch 11 train_loss= 0.39706963645109117 metrics= {'loss': 0.35826983068901463, 'logloss': 0.5863421204040492}
--------------------------------------------------
[[728.  36.   0.]
 [ 72.  94.   7.]
 [  1.  15.  29.]]
Precision: 0.7753768380657183 Recall: 0.6596232230345356 F1: 0.7109912722386851
Epoch 12 train_loss= 0.39125428244111243 metrics= {'loss': 0.31903020728591186, 'logloss': 0.6047503825451965}
New best model ! classification_right_neural_foraminal_narrowing.pth
--------------------------------------------------
[[681.  82.   1.]
 [ 46. 121.   6.]
 [  1.  15.  29.]]
Precision: 0.7525362179687213 Recall: 0.6954261378511598 F1: 0.716416832282295
Epoch 13 train_loss= 0.3676215379589846 metrics= {'loss': 0.34910048223381934, 'logloss': 0.5790737548315238}
--------------------------------------------------
[[699.  63.   2.]
 [ 59.  97.  17.]
 [  1.   8.  36.]]
Precision: 0.6705560485063591 Recall: 0.7480441070293686 F1: 0.7051078656797422
Epoch 14 train_loss= 0.3638817546480785 metrics= {'loss': 0.3439662433649903, 'logloss': 0.5901676853941445}
--------------------------------------------------
[[698.  66.   0.]
 [ 56. 109.   8.]
 [  1.  13.  31.]]
Precision: 0.7519378512615091 Recall: 0.7041833897052854 F1: 0.7255946942648538
Epoch 15 train_loss= 0.3513756569621876 metrics= {'loss': 0.33895553820293906, 'logloss': 0.6158541023638545}
--------------------------------------------------
[[700.  63.   1.]
 [ 52. 114.   7.]
 [  1.  14.  30.]]
Precision: 0.7544609840238569 Recall: 0.7001165869004745 F1: 0.7238839444876134
Epoch 16 train_loss= 0.33224845987072665 metrics= {'loss': 0.3432726078150828, 'logloss': 0.6118247252224547}
--------------------------------------------------
[[711.  53.   0.]
 [ 58. 105.  10.]
 [  1.  11.  33.]]
Precision: 0.7479639450761352 Recall: 0.7254049197077977 F1: 0.7364369202014162
Epoch 17 train_loss= 0.33052352831602577 metrics= {'loss': 0.3288761057509963, 'logloss': 0.5765960724771376}
--------------------------------------------------
[[701.  63.   0.]
 [ 62. 101.  10.]
 [  1.  11.  33.]]
Precision: 0.7345132033088408 Recall: 0.7169289511646441 F1: 0.7254941153207987
Epoch 18 train_loss= 0.3185518615894698 metrics= {'loss': 0.3391204919384564, 'logloss': 0.6007841492088293}
--------------------------------------------------
[[704.  60.   0.]
 [ 56. 107.  10.]
 [  1.  12.  32.]]
Precision: 0.7383211897727914 Recall: 0.7147006618120015 F1: 0.7259595021522663
Epoch 19 train_loss= 0.31872959977138376 metrics= {'loss': 0.3335188176515713, 'logloss': 0.5947567601147381}
--------------------------------------------------
[[562.  27.   5.]
 [ 81.  26.  43.]
 [ 15.  11.  47.]]
Precision: 0.5207929588408712 Recall: 0.5525910112211482 F1: 0.5174057321116019
Epoch 0 train_loss= 0.8352522834325621 metrics= {'loss': 0.5119925263808582, 'logloss': 1.0552520948858883}
New best model ! classification_right_subarticular_stenosis.pth
--------------------------------------------------
[[554.  33.   7.]
 [ 57.  34.  59.]
 [ 12.   7.  54.]]
Precision: 0.5154520721133883 Recall: 0.6206996250831868 F1: 0.536560046168407
Epoch 1 train_loss= 0.6989102957048952 metrics= {'loss': 0.4738712700065613, 'logloss': 0.9261449184724685}
New best model ! classification_right_subarticular_stenosis.pth
--------------------------------------------------
[[535.  48.  11.]
 [ 52.  48.  50.]
 [  8.  11.  54.]]
Precision: 0.5249452564447152 Recall: 0.6427967871803488 F1: 0.5635528497426386
Epoch 2 train_loss= 0.6391128355373991 metrics= {'loss': 0.4790726778022785, 'logloss': 0.8971221498390397}
--------------------------------------------------
[[561.  32.   1.]
 [ 60.  63.  27.]
 [  9.  14.  50.]]
Precision: 0.658648865333007 Recall: 0.6463100674059579 F1: 0.6483780374508852
Epoch 3 train_loss= 0.5970157879465462 metrics= {'loss': 0.42062072313740917, 'logloss': 0.9049380864181289}
New best model ! classification_right_subarticular_stenosis.pth
--------------------------------------------------
[[524.  62.   8.]
 [ 38.  74.  38.]
 [  3.  25.  45.]]
Precision: 0.5463871806750936 Recall: 0.6192249962112976 F1: 0.5787311746480963
Epoch 4 train_loss= 0.5550269963786649 metrics= {'loss': 0.4656977173604052, 'logloss': 0.858180197852996}
--------------------------------------------------
[[525.  64.   5.]
 [ 39.  64.  47.]
 [  5.  14.  54.]]
Precision: 0.5516879504303068 Recall: 0.6708679752515369 F1: 0.5989939484640155
Epoch 5 train_loss= 0.5473855216987437 metrics= {'loss': 0.4386079514231672, 'logloss': 0.8276877061327386}
--------------------------------------------------
[[549.  43.   2.]
 [ 48.  64.  38.]
 [  5.  15.  53.]]
Precision: 0.6058157647474767 Recall: 0.6688121923738362 F1: 0.6304933626752873
Epoch 6 train_loss= 0.5326947430310687 metrics= {'loss': 0.42416993828316074, 'logloss': 0.8478929719164192}
--------------------------------------------------
[[548.  45.   1.]
 [ 49.  75.  26.]
 [  6.  19.  48.]]
Precision: 0.6497037252926869 Recall: 0.6503851298371846 F1: 0.649754181091174
Epoch 7 train_loss= 0.5131925371799458 metrics= {'loss': 0.4109635466126392, 'logloss': 0.8311556163959344}
New best model ! classification_right_subarticular_stenosis.pth
--------------------------------------------------
[[526.  67.   1.]
 [ 35.  88.  27.]
 [  4.  18.  51.]]
Precision: 0.6472275675432171 Recall: 0.6933393952572035 F1: 0.6688104059570333
Epoch 8 train_loss= 0.49369513747363386 metrics= {'loss': 0.4351274532797026, 'logloss': 0.8221755979339078}
--------------------------------------------------
[[546.  44.   4.]
 [ 51.  57.  42.]
 [  4.  13.  56.]]
Precision: 0.5863663268968153 Recall: 0.6782407242681215 F1: 0.6196348421453024
Epoch 9 train_loss= 0.4855653617470327 metrics= {'loss': 0.41867601763864193, 'logloss': 0.8174026248236934}
--------------------------------------------------
[[527.  67.   0.]
 [ 34.  87.  29.]
 [  5.  18.  50.]]
Precision: 0.639195554708203 Recall: 0.6838473449432353 F1: 0.6601349969483571
Epoch 10 train_loss= 0.4629646283027753 metrics= {'loss': 0.41495271992647814, 'logloss': 0.8047662268905282}
--------------------------------------------------
[[529.  64.   1.]
 [ 35.  97.  18.]
 [  3.  22.  48.]]
Precision: 0.6941087901508283 Recall: 0.687720387172442 F1: 0.6884722406317091
Epoch 11 train_loss= 0.45138398187827167 metrics= {'loss': 0.4212263314124448, 'logloss': 0.8211369080385797}
--------------------------------------------------
[[533.  61.   0.]
 [ 41.  91.  18.]
 [  5.  20.  48.]]
Precision: 0.6982547315719988 Recall: 0.6772538167058715 F1: 0.6859722306188661
Epoch 12 train_loss= 0.43117812195104455 metrics= {'loss': 0.3986128736891646, 'logloss': 0.822511072271539}
New best model ! classification_right_subarticular_stenosis.pth
--------------------------------------------------
[[531.  63.   0.]
 [ 30.  98.  22.]
 [  4.  21.  48.]]
Precision: 0.6799433183756823 Recall: 0.6901061495582044 F1: 0.6831921054609911
Epoch 13 train_loss= 0.4437069313064138 metrics= {'loss': 0.4120734533963857, 'logloss': 0.8118280759999472}
--------------------------------------------------
[[549.  44.   1.]
 [ 44.  82.  24.]
 [  6.  16.  51.]]
Precision: 0.6793810928129184 Recall: 0.6874423293601376 F1: 0.6831302175565056
Epoch 14 train_loss= 0.4113938439839884 metrics= {'loss': 0.39589314743857196, 'logloss': 0.8429527853906242}
New best model ! classification_right_subarticular_stenosis.pth
--------------------------------------------------
[[534.  60.   0.]
 [ 33. 104.  13.]
 [  4.  26.  43.]]
Precision: 0.7287666877978032 Recall: 0.6631172784597441 F1: 0.6867048965508952
Epoch 15 train_loss= 0.4049747076351196 metrics= {'loss': 0.40631698531571653, 'logloss': 0.8560418741443306}
--------------------------------------------------
[[535.  58.   1.]
 [ 36.  94.  20.]
 [  3.  21.  49.]]
Precision: 0.688394421634437 Recall: 0.6912768915508641 F1: 0.6887773312823146
Epoch 16 train_loss= 0.3911993210345668 metrics= {'loss': 0.4161221845413268, 'logloss': 0.8784519586444893}
--------------------------------------------------
[[521.  71.   2.]
 [ 28.  93.  29.]
 [  4.  17.  52.]]
Precision: 0.6393974601899064 Recall: 0.7094884922282182 F1: 0.6712843685848267
Epoch 17 train_loss= 0.3854282535157042 metrics= {'loss': 0.438951471091669, 'logloss': 0.8453521318318086}
--------------------------------------------------
[[543.  50.   1.]
 [ 42.  78.  30.]
 [  5.  18.  50.]]
Precision: 0.6368525623149878 Recall: 0.670552491648382 F1: 0.652669483026626
Epoch 18 train_loss= 0.3551310037000803 metrics= {'loss': 0.4411466622287178, 'logloss': 0.9713437413193295}
--------------------------------------------------
[[547.  46.   1.]
 [ 44.  89.  17.]
 [  3.  22.  48.]]
Precision: 0.709103470249967 Recall: 0.6768112962633511 F1: 0.6918677813505925
Epoch 19 train_loss= 0.34588542823899915 metrics= {'loss': 0.41113278357539906, 'logloss': 0.8831762275282795}
--------------------------------------------------
Error label nan
[[579.  11.   8.]
 [ 96.  16.  38.]
 [ 14.   3.  49.]]
Precision: 0.5671675560454111 Recall: 0.5930368182542096 F1: 0.5271578662883011
Epoch 0 train_loss= 0.8519213364488036 metrics= {'loss': 0.4852836555984393, 'logloss': 0.9816000007790301}
New best model ! classification_left_subarticular_stenosis.pth
--------------------------------------------------
[[576.  19.   3.]
 [ 92.  27.  31.]
 [  6.   9.  51.]]
Precision: 0.6052025126209103 Recall: 0.6305885418928897 F1: 0.5906394858025801
Epoch 1 train_loss= 0.6895853074666063 metrics= {'loss': 0.4168925634376517, 'logloss': 0.8376267085864989}
New best model ! classification_left_subarticular_stenosis.pth
--------------------------------------------------
[[540.  52.   6.]
 [ 48.  76.  26.]
 [  2.  10.  54.]]
Precision: 0.6469044885181712 Recall: 0.7412958056436317 F1: 0.6866788182577658
Epoch 2 train_loss= 0.6199670890844367 metrics= {'loss': 0.404999381387838, 'logloss': 0.7402869102291085}
New best model ! classification_left_subarticular_stenosis.pth
--------------------------------------------------
[[526.  66.   6.]
 [ 37.  83.  30.]
 [  2.   8.  56.]]
Precision: 0.6318687001124881 Recall: 0.7686006746876313 F1: 0.6887762060625766
Epoch 3 train_loss= 0.5809112885040661 metrics= {'loss': 0.41227025766586667, 'logloss': 0.6945873050687246}
--------------------------------------------------
[[550.  43.   5.]
 [ 58.  71.  21.]
 [  3.  11.  52.]]
Precision: 0.6718329046839686 Recall: 0.7168448942361986 F1: 0.6902084295632681
Epoch 4 train_loss= 0.5416151748026625 metrics= {'loss': 0.39281881164932414, 'logloss': 0.7326659866302284}
New best model ! classification_left_subarticular_stenosis.pth
--------------------------------------------------
[[530.  66.   2.]
 [ 40.  94.  16.]
 [  2.  10.  54.]]
Precision: 0.7189222542163718 Recall: 0.7731926044969525 F1: 0.7444882412273717
Epoch 5 train_loss= 0.5238917993654102 metrics= {'loss': 0.3777832543165354, 'logloss': 0.6631923839841032}
New best model ! classification_left_subarticular_stenosis.pth
--------------------------------------------------
[[549.  48.   1.]
 [ 44.  94.  12.]
 [  2.  16.  48.]]
Precision: 0.7514433538511897 Recall: 0.7257834918704484 F1: 0.7378242980184188
Epoch 6 train_loss= 0.49621302498744474 metrics= {'loss': 0.3505223938591192, 'logloss': 0.6569334183501577}
New best model ! classification_left_subarticular_stenosis.pth
--------------------------------------------------
[[548.  50.   0.]
 [ 53.  83.  14.]
 [  2.  13.  51.]]
Precision: 0.7406053180332469 Recall: 0.730566245348854 F1: 0.7355283277669838
Epoch 7 train_loss= 0.488568375631862 metrics= {'loss': 0.356675266359443, 'logloss': 0.67521055289522}
--------------------------------------------------
[[546.  51.   1.]
 [ 45.  86.  19.]
 [  2.   9.  55.]]
Precision: 0.7188796449994445 Recall: 0.7704347826086957 F1: 0.742800873193713
Epoch 8 train_loss= 0.46192733482960907 metrics= {'loss': 0.3593716810378249, 'logloss': 0.6538068550449072}
--------------------------------------------------
[[541.  54.   3.]
 [ 43.  84.  23.]
 [  2.   7.  57.]]
Precision: 0.6898309760840974 Recall: 0.7827468183989923 F1: 0.7304624409660809
Epoch 9 train_loss= 0.462096112814842 metrics= {'loss': 0.36255215854043965, 'logloss': 0.6387356195635945}
--------------------------------------------------
[[528.  68.   2.]
 [ 32.  98.  20.]
 [  2.   8.  56.]]
Precision: 0.7053904903947854 Recall: 0.7976498863455385 F1: 0.7473332117010278
Epoch 10 train_loss= 0.44376576246869665 metrics= {'loss': 0.37452831984311974, 'logloss': 0.6114341429358489}
--------------------------------------------------
[[539.  58.   1.]
 [ 36. 100.  14.]
 [  2.   9.  55.]]
Precision: 0.753514863951579 Recall: 0.7954292084726867 F1: 0.7735100827363632
Epoch 11 train_loss= 0.4177486356764658 metrics= {'loss': 0.3448901833421911, 'logloss': 0.615799699065688}
New best model ! classification_left_subarticular_stenosis.pth
--------------------------------------------------
[[553.  45.   0.]
 [ 49.  88.  13.]
 [  2.  12.  52.]]
Precision: 0.7613365739079373 Recall: 0.749942521246869 F1: 0.7555608527925551
Epoch 12 train_loss= 0.4131443537436185 metrics= {'loss': 0.350218036147806, 'logloss': 0.6712947268968009}
--------------------------------------------------
[[549.  48.   1.]
 [ 49.  86.  15.]
 [  2.  10.  54.]]
Precision: 0.7421655328798187 Recall: 0.7624934485804051 F1: 0.7518665944794327
Epoch 13 train_loss= 0.4074425850304266 metrics= {'loss': 0.34051646950371833, 'logloss': 0.6261214756671907}
New best model ! classification_left_subarticular_stenosis.pth
--------------------------------------------------
[[532.  64.   2.]
 [ 36.  92.  22.]
 [  2.   9.  55.]]
Precision: 0.6904707107238754 Recall: 0.778518872431916 F1: 0.7305279472834638
Epoch 14 train_loss= 0.3808329063193166 metrics= {'loss': 0.3594981396161873, 'logloss': 0.6215644659482867}
--------------------------------------------------
[[549.  48.   1.]
 [ 50.  77.  23.]
 [  3.   5.  58.]]
Precision: 0.7036919728840967 Recall: 0.7799826260695826 F1: 0.7357335907335908
Epoch 15 train_loss= 0.3824514651634956 metrics= {'loss': 0.35750204921061707, 'logloss': 0.6518262828041755}
--------------------------------------------------
[[549.  47.   2.]
 [ 58.  66.  26.]
 [  2.   6.  58.]]
Precision: 0.6726279926565655 Recall: 0.7590302451172016 F1: 0.7062482390284714
Epoch 16 train_loss= 0.36549179351280614 metrics= {'loss': 0.36331357325660657, 'logloss': 0.6776365916450078}
--------------------------------------------------
[[540.  57.   1.]
 [ 47.  89.  14.]
 [  2.  14.  50.]]
Precision: 0.7294616037612643 Recall: 0.7314256757735019 F1: 0.7302398561544106
Epoch 17 train_loss= 0.35840803859840575 metrics= {'loss': 0.3628711842956239, 'logloss': 0.685781393837521}
--------------------------------------------------
[[522.  73.   3.]
 [ 28. 102.  20.]
 [  1.  10.  55.]]
Precision: 0.6957977063240222 Recall: 0.7951775760471413 F1: 0.7402974559009008
Epoch 18 train_loss= 0.3497377038856067 metrics= {'loss': 0.38032146016437685, 'logloss': 0.6310472583148355}
--------------------------------------------------
